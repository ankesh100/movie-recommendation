{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "deep learning.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankesh100/movie-recommendation/blob/main/deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "974b4188",
        "outputId": "03ef95f7-fada-42fe-f719-1cdead390d59"
      },
      "source": [
        "# @title Imports (run this cell)\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from IPython import display\n",
        "from matplotlib import pyplot as plt\n",
        "import sklearn\n",
        "import sklearn.manifold\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "# Add some convenience functions to Pandas DataFrame.\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = '{:.3f}'.format\n",
        "def mask(df, key, function):\n",
        "  \"\"\"Returns a filtered dataframe, by applying function to key\"\"\"\n",
        "  return df[function(df[key])]\n",
        "\n",
        "def flatten_cols(df):\n",
        "  df.columns = [' '.join(col).strip() for col in df.columns.values]\n",
        "  return df\n",
        "\n",
        "pd.DataFrame.mask = mask\n",
        "pd.DataFrame.flatten_cols = flatten_cols\n",
        "\n",
        "# Install Altair and activate its colab renderer.\n",
        "print(\"Installing Altair...\")\n",
        "!pip install git+git://github.com/altair-viz/altair.git\n",
        "import altair as alt\n",
        "alt.data_transformers.enable('default', max_rows=None)\n",
        "alt.renderers.enable('colab')\n",
        "print(\"Done installing Altair.\")\n",
        "\n",
        "# Install spreadsheets and import authentication module.\n",
        "USER_RATINGS = False\n",
        "!pip install --upgrade -q gspread\n",
        "from google.colab import auth\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "id": "974b4188",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/ankesh/.local/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "Installing Altair...\n",
            "Collecting git+git://github.com/altair-viz/altair.git\n",
            "  Cloning git://github.com/altair-viz/altair.git to /tmp/pip-req-build-ur83fak7\n",
            "  Running command git clone -q git://github.com/altair-viz/altair.git /tmp/pip-req-build-ur83fak7\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.18 in ./.local/lib/python3.9/site-packages (from altair==4.2.0.dev0) (1.2.4)\n",
            "Requirement already satisfied: entrypoints in ./.local/lib/python3.9/site-packages (from altair==4.2.0.dev0) (0.3)\n",
            "Requirement already satisfied: jinja2 in ./.local/lib/python3.9/site-packages (from altair==4.2.0.dev0) (3.0.1)\n",
            "Requirement already satisfied: jsonschema in ./.local/lib/python3.9/site-packages (from altair==4.2.0.dev0) (3.2.0)\n",
            "Requirement already satisfied: numpy in ./.local/lib/python3.9/site-packages (from altair==4.2.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: toolz in ./.local/lib/python3.9/site-packages (from altair==4.2.0.dev0) (0.11.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3/dist-packages (from pandas>=0.18->altair==4.2.0.dev0) (2021.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/lib/python3/dist-packages (from pandas>=0.18->altair==4.2.0.dev0) (2.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.local/lib/python3.9/site-packages (from jinja2->altair==4.2.0.dev0) (2.0.1)\n",
            "Requirement already satisfied: six>=1.11.0 in ./.local/lib/python3.9/site-packages (from jsonschema->altair==4.2.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in ./.local/lib/python3.9/site-packages (from jsonschema->altair==4.2.0.dev0) (0.17.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in ./.local/lib/python3.9/site-packages (from jsonschema->altair==4.2.0.dev0) (21.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from jsonschema->altair==4.2.0.dev0) (52.0.0)\n",
            "Done installing Altair.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-491fc79da902>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mUSER_RATINGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install --upgrade -q gspread'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgspread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moauth2client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGoogleCredentials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fc0e61e",
        "outputId": "02956bd2-bfe5-4b82-db4a-89dc593d286b"
      },
      "source": [
        "# @title Load the MovieLens data (run this cell).\n",
        "\n",
        "# Download MovieLens data.\n",
        "print(\"Downloading movielens data...\")\n",
        "from urllib.request import urlretrieve\n",
        "import zipfile\n",
        "\n",
        "urlretrieve(\"http://files.grouplens.org/datasets/movielens/ml-100k.zip\", \"movielens.zip\")\n",
        "zip_ref = zipfile.ZipFile('movielens.zip', \"r\")\n",
        "zip_ref.extractall()\n",
        "print(\"Done. Dataset contains:\")\n",
        "print(zip_ref.read('ml-100k/u.info'))\n",
        "\n",
        "# Load each data set (users, movies, and ratings).\n",
        "users_cols = ['user_id', 'age', 'sex', 'occupation', 'zip_code']\n",
        "users = pd.read_csv(\n",
        "    'ml-100k/u.user', sep='|', names=users_cols, encoding='latin-1')\n",
        "\n",
        "ratings_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
        "ratings = pd.read_csv(\n",
        "    'ml-100k/u.data', sep='\\t', names=ratings_cols, encoding='latin-1')\n",
        "\n",
        "# The movies file contains a binary feature for each genre.\n",
        "genre_cols = [\n",
        "    \"genre_unknown\", \"Action\", \"Adventure\", \"Animation\", \"Children\", \"Comedy\",\n",
        "    \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\",\n",
        "    \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"\n",
        "]\n",
        "movies_cols = [\n",
        "    'movie_id', 'title', 'release_date', \"video_release_date\", \"imdb_url\"\n",
        "] + genre_cols\n",
        "movies = pd.read_csv(\n",
        "    'ml-100k/u.item', sep='|', names=movies_cols, encoding='latin-1')\n",
        "\n",
        "# Since the ids start at 1, we shift them to start at 0.\n",
        "users[\"user_id\"] = users[\"user_id\"].apply(lambda x: str(x-1))\n",
        "movies[\"movie_id\"] = movies[\"movie_id\"].apply(lambda x: str(x-1))\n",
        "movies[\"year\"] = movies['release_date'].apply(lambda x: str(x).split('-')[-1])\n",
        "ratings[\"movie_id\"] = ratings[\"movie_id\"].apply(lambda x: str(x-1))\n",
        "ratings[\"user_id\"] = ratings[\"user_id\"].apply(lambda x: str(x-1))\n",
        "ratings[\"rating\"] = ratings[\"rating\"].apply(lambda x: float(x))\n",
        "\n",
        "# Compute the number of movies to which a genre is assigned.\n",
        "genre_occurences = movies[genre_cols].sum().to_dict()\n",
        "\n",
        "# Since some movies can belong to more than one genre, we create different\n",
        "# 'genre' columns as follows:\n",
        "# - all_genres: all the active genres of the movie.\n",
        "# - genre: randomly sampled from the active genres.\n",
        "def mark_genres(movies, genres):\n",
        "  def get_random_genre(gs):\n",
        "    active = [genre for genre, g in zip(genres, gs) if g==1]\n",
        "    if len(active) == 0:\n",
        "      return 'Other'\n",
        "    return np.random.choice(active)\n",
        "  def get_all_genres(gs):\n",
        "    active = [genre for genre, g in zip(genres, gs) if g==1]\n",
        "    if len(active) == 0:\n",
        "      return 'Other'\n",
        "    return '-'.join(active)\n",
        "  movies['genre'] = [\n",
        "      get_random_genre(gs) for gs in zip(*[movies[genre] for genre in genres])]\n",
        "  movies['all_genres'] = [\n",
        "      get_all_genres(gs) for gs in zip(*[movies[genre] for genre in genres])]\n",
        "\n",
        "mark_genres(movies, genre_cols)\n",
        "\n",
        "# Create one merged DataFrame containing all the movielens data.\n",
        "movielens = ratings.merge(movies, on='movie_id').merge(users, on='user_id')\n",
        "\n",
        "# Utility to split the data into training and test sets.\n",
        "def split_dataframe(df, holdout_fraction=0.1):\n",
        "  \"\"\"Splits a DataFrame into training and test sets.\n",
        "  Args:\n",
        "    df: a dataframe.\n",
        "    holdout_fraction: fraction of dataframe rows to use in the test set.\n",
        "  Returns:\n",
        "    train: dataframe for training\n",
        "    test: dataframe for testing\n",
        "  \"\"\"\n",
        "  test = df.sample(frac=holdout_fraction, replace=False)\n",
        "  train = df[~df.index.isin(test.index)]\n",
        "  return train, test"
      ],
      "id": "9fc0e61e",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading movielens data...\n",
            "Done. Dataset contains:\n",
            "b'943 users\\n1682 items\\n100000 ratings\\n'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd723d24"
      },
      "source": [
        "# @title CFModel helper class (run this cell)\n",
        "class CFModel(object):\n",
        "  \"\"\"Simple class that represents a collaborative filtering model\"\"\"\n",
        "  def __init__(self, embedding_vars, loss, metrics=None):\n",
        "    \"\"\"Initializes a CFModel.\n",
        "    Args:\n",
        "      embedding_vars: A dictionary of tf.Variables.\n",
        "      loss: A float Tensor. The loss to optimize.\n",
        "      metrics: optional list of dictionaries of Tensors. The metrics in each\n",
        "        dictionary will be plotted in a separate figure during training.\n",
        "    \"\"\"\n",
        "    self._embedding_vars = embedding_vars\n",
        "    self._loss = loss\n",
        "    self._metrics = metrics\n",
        "    self._embeddings = {k: None for k in embedding_vars}\n",
        "    self._session = None\n",
        "\n",
        "  @property\n",
        "  def embeddings(self):\n",
        "    \"\"\"The embeddings dictionary.\"\"\"\n",
        "    return self._embeddings\n",
        "\n",
        "  def train(self, num_iterations=100, learning_rate=1.0, plot_results=True,\n",
        "            optimizer=tf.train.GradientDescentOptimizer):\n",
        "    \"\"\"Trains the model.\n",
        "    Args:\n",
        "      iterations: number of iterations to run.\n",
        "      learning_rate: optimizer learning rate.\n",
        "      plot_results: whether to plot the results at the end of training.\n",
        "      optimizer: the optimizer to use. Default to GradientDescentOptimizer.\n",
        "    Returns:\n",
        "      The metrics dictionary evaluated at the last iteration.\n",
        "    \"\"\"\n",
        "    with self._loss.graph.as_default():\n",
        "      opt = optimizer(learning_rate)\n",
        "      train_op = opt.minimize(self._loss)\n",
        "      local_init_op = tf.group(\n",
        "          tf.variables_initializer(opt.variables()),\n",
        "          tf.local_variables_initializer())\n",
        "      if self._session is None:\n",
        "        self._session = tf.Session()\n",
        "        with self._session.as_default():\n",
        "          self._session.run(tf.global_variables_initializer())\n",
        "          self._session.run(tf.tables_initializer())\n",
        "          tf.train.start_queue_runners()\n",
        "\n",
        "    with self._session.as_default():\n",
        "      local_init_op.run()\n",
        "      iterations = []\n",
        "      metrics = self._metrics or ({},)\n",
        "      metrics_vals = [collections.defaultdict(list) for _ in self._metrics]\n",
        "\n",
        "      # Train and append results.\n",
        "      for i in range(num_iterations + 1):\n",
        "        _, results = self._session.run((train_op, metrics))\n",
        "        if (i % 10 == 0) or i == num_iterations:\n",
        "          print(\"\\r iteration %d: \" % i + \", \".join(\n",
        "                [\"%s=%f\" % (k, v) for r in results for k, v in r.items()]),\n",
        "                end='')\n",
        "          iterations.append(i)\n",
        "          for metric_val, result in zip(metrics_vals, results):\n",
        "            for k, v in result.items():\n",
        "              metric_val[k].append(v)\n",
        "\n",
        "      for k, v in self._embedding_vars.items():\n",
        "        self._embeddings[k] = v.eval()\n",
        "\n",
        "      if plot_results:\n",
        "        # Plot the metrics.\n",
        "        num_subplots = len(metrics)+1\n",
        "        fig = plt.figure()\n",
        "        fig.set_size_inches(num_subplots*10, 8)\n",
        "        for i, metric_vals in enumerate(metrics_vals):\n",
        "          ax = fig.add_subplot(1, num_subplots, i+1)\n",
        "          for k, v in metric_vals.items():\n",
        "            ax.plot(iterations, v, label=k)\n",
        "          ax.set_xlim([1, num_iterations])\n",
        "          ax.legend()\n",
        "      return results"
      ],
      "id": "dd723d24",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c8424a5"
      },
      "source": [
        "#@title Solution\n",
        "def build_model(ratings, embedding_dim=3, init_stddev=1.):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    ratings: a DataFrame of the ratings\n",
        "    embedding_dim: the dimension of the embedding vectors.\n",
        "    init_stddev: float, the standard deviation of the random initial embeddings.\n",
        "  Returns:\n",
        "    model: a CFModel.\n",
        "  \"\"\"\n",
        "  # Split the ratings DataFrame into train and test.\n",
        "  train_ratings, test_ratings = split_dataframe(ratings)\n",
        "  # SparseTensor representation of the train and test datasets.\n",
        "  A_train = build_rating_sparse_tensor(train_ratings)\n",
        "  A_test = build_rating_sparse_tensor(test_ratings)\n",
        "  # Initialize the embeddings using a normal distribution.\n",
        "  U = tf.Variable(tf.random_normal(\n",
        "      [A_train.dense_shape[0], embedding_dim], stddev=init_stddev))\n",
        "  V = tf.Variable(tf.random_normal(\n",
        "      [A_train.dense_shape[1], embedding_dim], stddev=init_stddev))\n",
        "  train_loss = sparse_mean_square_error(A_train, U, V)\n",
        "  test_loss = sparse_mean_square_error(A_test, U, V)\n",
        "  metrics = {\n",
        "      'train_error': train_loss,\n",
        "      'test_error': test_loss\n",
        "  }\n",
        "  embeddings = {\n",
        "      \"user_id\": U,\n",
        "      \"movie_id\": V\n",
        "  }\n",
        "  return CFModel(embeddings, train_loss, [metrics])"
      ],
      "id": "2c8424a5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41caa542",
        "outputId": "97ec692b-88cc-4a93-b016-5a03f0e70ec2"
      },
      "source": [
        "rated_movies = (ratings[[\"user_id\", \"movie_id\"]]\n",
        "                .groupby(\"user_id\", as_index=False)\n",
        "                .aggregate(lambda x: list(x)))\n",
        "rated_movies.head()"
      ],
      "id": "41caa542",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>movie_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[60, 188, 32, 159, 19, 201, 170, 264, 154, 116...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[291, 250, 49, 313, 296, 289, 311, 280, 12, 27...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10</td>\n",
              "      <td>[110, 557, 731, 226, 424, 739, 722, 37, 724, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100</td>\n",
              "      <td>[828, 303, 595, 221, 470, 404, 280, 251, 281, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>101</td>\n",
              "      <td>[767, 822, 69, 514, 523, 321, 624, 160, 447, 4...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  user_id                                           movie_id\n",
              "0       0  [60, 188, 32, 159, 19, 201, 170, 264, 154, 116...\n",
              "1       1  [291, 250, 49, 313, 296, 289, 311, 280, 12, 27...\n",
              "2      10  [110, 557, 731, 226, 424, 739, 722, 37, 724, 1...\n",
              "3     100  [828, 303, 595, 221, 470, 404, 280, 251, 281, ...\n",
              "4     101  [767, 822, 69, 514, 523, 321, 624, 160, 447, 4..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c3e765a"
      },
      "source": [
        "#@title Batch generation code (run this cell)\n",
        "years_dict = {\n",
        "    movie: year for movie, year in zip(movies[\"movie_id\"], movies[\"year\"])\n",
        "}\n",
        "genres_dict = {\n",
        "    movie: genres.split('-')\n",
        "    for movie, genres in zip(movies[\"movie_id\"], movies[\"all_genres\"])\n",
        "}\n",
        "\n",
        "def make_batch(ratings, batch_size):\n",
        "  \"\"\"Creates a batch of examples.\n",
        "  Args:\n",
        "    ratings: A DataFrame of ratings such that examples[\"movie_id\"] is a list of\n",
        "      movies rated by a user.\n",
        "    batch_size: The batch size.\n",
        "  \"\"\"\n",
        "  def pad(x, fill):\n",
        "    return pd.DataFrame.from_dict(x).fillna(fill).values\n",
        "\n",
        "  movie = []\n",
        "  year = []\n",
        "  genre = []\n",
        "  label = []\n",
        "  for movie_ids in ratings[\"movie_id\"].values:\n",
        "    movie.append(movie_ids)\n",
        "    genre.append([x for movie_id in movie_ids for x in genres_dict[movie_id]])\n",
        "    year.append([years_dict[movie_id] for movie_id in movie_ids])\n",
        "    label.append([int(movie_id) for movie_id in movie_ids])\n",
        "  features = {\n",
        "      \"movie_id\": pad(movie, \"\"),\n",
        "      \"year\": pad(year, \"\"),\n",
        "      \"genre\": pad(genre, \"\"),\n",
        "      \"label\": pad(label, -1)\n",
        "  }\n",
        "  batch = (\n",
        "      tf.data.Dataset.from_tensor_slices(features)\n",
        "      .shuffle(1000)\n",
        "      .repeat()\n",
        "      .batch(batch_size)\n",
        "      .make_one_shot_iterator()\n",
        "      .get_next())\n",
        "  return batch\n",
        "\n",
        "def select_random(x):\n",
        "  \"\"\"Selectes a random elements from each row of x.\"\"\"\n",
        "  def to_float(x):\n",
        "    return tf.cast(x, tf.float32)\n",
        "  def to_int(x):\n",
        "    return tf.cast(x, tf.int64)\n",
        "  batch_size = tf.shape(x)[0]\n",
        "  rn = tf.range(batch_size)\n",
        "  nnz = to_float(tf.count_nonzero(x >= 0, axis=1))\n",
        "  rnd = tf.random_uniform([batch_size])\n",
        "  ids = tf.stack([to_int(rn), to_int(nnz * rnd)], axis=1)\n",
        "  return to_int(tf.gather_nd(x, ids))\n"
      ],
      "id": "3c3e765a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0232712f"
      },
      "source": [
        "# @title Solution\n",
        "def softmax_loss(user_embeddings, movie_embeddings, labels):\n",
        "  \"\"\"Returns the cross-entropy loss of the softmax model.\n",
        "  Args:\n",
        "    user_embeddings: A tensor of shape [batch_size, embedding_dim].\n",
        "    movie_embeddings: A tensor of shape [num_movies, embedding_dim].\n",
        "    labels: A tensor of [batch_size], such that labels[i] is the target label\n",
        "      for example i.\n",
        "  Returns:\n",
        "    The mean cross-entropy loss.\n",
        "  \"\"\"\n",
        "  # Verify that the embddings have compatible dimensions\n",
        "  user_emb_dim = user_embeddings.shape[1].value\n",
        "  movie_emb_dim = movie_embeddings.shape[1].value\n",
        "  if user_emb_dim != movie_emb_dim:\n",
        "    raise ValueError(\n",
        "        \"The user embedding dimension %d should match the movie embedding \"\n",
        "        \"dimension % d\" % (user_emb_dim, movie_emb_dim))\n",
        "\n",
        "  logits = tf.matmul(user_embeddings, movie_embeddings, transpose_b=True)\n",
        "  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "      logits=logits, labels=labels))\n",
        "  return loss"
      ],
      "id": "0232712f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "552de06c"
      },
      "source": [
        "# @title Solution\n",
        "\n",
        "def build_softmax_model(rated_movies, embedding_cols, hidden_dims):\n",
        "  \"\"\"Builds a Softmax model for MovieLens.\n",
        "  Args:\n",
        "    rated_movies: DataFrame of traing examples.\n",
        "    embedding_cols: A dictionary mapping feature names (string) to embedding\n",
        "      column objects. This will be used in tf.feature_column.input_layer() to\n",
        "      create the input layer.\n",
        "    hidden_dims: int list of the dimensions of the hidden layers.\n",
        "  Returns:\n",
        "    A CFModel object.\n",
        "  \"\"\"\n",
        "  def create_network(features):\n",
        "    \"\"\"Maps input features dictionary to user embeddings.\n",
        "    Args:\n",
        "      features: A dictionary of input string tensors.\n",
        "    Returns:\n",
        "      outputs: A tensor of shape [batch_size, embedding_dim].\n",
        "    \"\"\"\n",
        "    # Create a bag-of-words embedding for each sparse feature.\n",
        "    inputs = tf.feature_column.input_layer(features, embedding_cols)\n",
        "    # Hidden layers.\n",
        "    input_dim = inputs.shape[1].value\n",
        "    for i, output_dim in enumerate(hidden_dims):\n",
        "      w = tf.get_variable(\n",
        "          \"hidden%d_w_\" % i, shape=[input_dim, output_dim],\n",
        "          initializer=tf.truncated_normal_initializer(\n",
        "              stddev=1./np.sqrt(output_dim))) / 10.\n",
        "      outputs = tf.matmul(inputs, w)\n",
        "      input_dim = output_dim\n",
        "      inputs = outputs\n",
        "    return outputs\n",
        "\n",
        "  train_rated_movies, test_rated_movies = split_dataframe(rated_movies)\n",
        "  train_batch = make_batch(train_rated_movies, 200)\n",
        "  test_batch = make_batch(test_rated_movies, 100)\n",
        "\n",
        "  with tf.variable_scope(\"model\", reuse=False):\n",
        "    # Train\n",
        "    train_user_embeddings = create_network(train_batch)\n",
        "    train_labels = select_random(train_batch[\"label\"])\n",
        "  with tf.variable_scope(\"model\", reuse=True):\n",
        "    # Test\n",
        "    test_user_embeddings = create_network(test_batch)\n",
        "    test_labels = select_random(test_batch[\"label\"])\n",
        "    movie_embeddings = tf.get_variable(\n",
        "        \"input_layer/movie_id_embedding/embedding_weights\")\n",
        "\n",
        "  test_loss = softmax_loss(\n",
        "      test_user_embeddings, movie_embeddings, test_labels)\n",
        "  train_loss = softmax_loss(\n",
        "      train_user_embeddings, movie_embeddings, train_labels)\n",
        "  _, test_precision_at_10 = tf.metrics.precision_at_k(\n",
        "      labels=test_labels,\n",
        "      predictions=tf.matmul(test_user_embeddings, movie_embeddings, transpose_b=True),\n",
        "      k=10)\n",
        "\n",
        "  metrics = (\n",
        "      {\"train_loss\": train_loss, \"test_loss\": test_loss},\n",
        "      {\"test_precision_at_10\": test_precision_at_10}\n",
        "  )\n",
        "  embeddings = {\"movie_id\": movie_embeddings}\n",
        "  return CFModel(embeddings, train_loss, metrics)\n"
      ],
      "id": "552de06c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "836b1ee3"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "id": "836b1ee3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fa772ef",
        "outputId": "96b5f0ce-14ef-4839-aea5-ed87d8329ca0"
      },
      "source": [
        "# Create feature embedding columns\n",
        "def make_embedding_col(key, embedding_dim):\n",
        "  categorical_col = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "      key=key, vocabulary_list=list(set(movies[key].values)), num_oov_buckets=0)\n",
        "  return tf.feature_column.embedding_column(\n",
        "      categorical_column=categorical_col, dimension=embedding_dim,\n",
        "      # default initializer: trancated normal with stddev=1/sqrt(dimension)\n",
        "      combiner='mean')\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "  softmax_model = build_softmax_model(\n",
        "      rated_movies,\n",
        "      embedding_cols=[\n",
        "          make_embedding_col(\"movie_id\", 35),\n",
        "          make_embedding_col(\"genre\", 3),\n",
        "          make_embedding_col(\"year\", 2),\n",
        "      ],\n",
        "      hidden_dims=[35])\n",
        "\n",
        "softmax_model.train(\n",
        "    learning_rate=8., num_iterations=3000, optimizer=tf.train.AdagradOptimizer)"
      ],
      "id": "1fa772ef",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'BatchDataset' object has no attribute 'make_one_shot_iterator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-f37cf5b1087e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   softmax_model = build_softmax_model(\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0mrated_movies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       embedding_cols=[\n",
            "\u001b[0;32m<ipython-input-14-0097c377f6a6>\u001b[0m in \u001b[0;36mbuild_softmax_model\u001b[0;34m(rated_movies, embedding_cols, hidden_dims)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0mtrain_rated_movies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_rated_movies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrated_movies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m   \u001b[0mtrain_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_rated_movies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m   \u001b[0mtest_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_rated_movies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-da73a6a05308>\u001b[0m in \u001b[0;36mmake_batch\u001b[0;34m(ratings, batch_size)\u001b[0m\n\u001b[1;32m     34\u001b[0m   }\n\u001b[1;32m     35\u001b[0m   batch = (\n\u001b[0;32m---> 36\u001b[0;31m       \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m       \u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'BatchDataset' object has no attribute 'make_one_shot_iterator'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8f5e473"
      },
      "source": [
        ""
      ],
      "id": "b8f5e473",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d87afe1"
      },
      "source": [
        ""
      ],
      "id": "9d87afe1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2da03b8"
      },
      "source": [
        ""
      ],
      "id": "d2da03b8",
      "execution_count": null,
      "outputs": []
    }
  ]
}